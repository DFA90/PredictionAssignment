---
title: "MachineLearning: Human Activity Recognition"
author: "DFA"
date: '`r Sys.Date()`'
output:
  html_document:
    code_folding: show
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
    hide: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, message=FALSE}
library(plyr)
library(dplyr)
library(caret)
library(randomForest)
library(gbm)
library(MASS)
# set.seed(62433)
```

# Instroduction

This analysis is related to the work done in [WeightLiftingExerciseDataset](http://groupware.les.inf.puc-rio.br/har). Target was to "define quality of execution and investigate three aspects that pertain to qualitative activity recognition: the problem of specifying correct execution, the automatic and robust detection of execution mistakes, and how to provide feedback on the quality of execution to the user" using a "on-body sensing approach".

We took the Weight-Lifting-Exercise-Dataset and trained a model on a trainingset to predict the quality of the testingset.



# Preprocessing

## Download Files
```{r, cache=T}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "fitTrain.csv")
training = read.csv("fitTrain.csv")

download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "fitTest.csv")
testing = read.csv("fitTest.csv")
```

## Split Data for Cross Validation
To test our modelfits we split up (80:20) our training set to apply cross-validation.
```{r}
inTrain <- createDataPartition(y = training$classe, p = 0.8, list = F)
trainingTrain <- training[inTrain,]
trainingTest <- training[-inTrain,]
```


## Cleanup Data
Some columns provided have to be removed. For example the name of the testpersons as well the the time can't be used to predict for a random person at a random time. Furthermore some columns might contain at least 80% NA. These data isn't reliable and therefore removed.
```{r cleanup}
trainNoTime <- trainingTrain[,-c(1:7)] #remove inapprpriate columns

na80 <- apply(trainNoTime, 2, function(i){mean(is.na(i)) > 0.8}) %>% as.vector() #find + remove > 80% NA
trainNA <- trainNoTime[, !na80]
```

Further preprocessing is done automatically by *preProcess*. First remove columns with variance close to zero (*nzv*), then apply *center*, *scale* and *pca*.
```{r preProcess}
(preProc <- preProcess(trainNA[,-ncol(trainNA)], method = c("center", "scale", "nzv", "pca"), thresh = 0.90))
```

Apply preprossing to training set. Use only new features from preprocessing!
```{r trainTraining}
trainPC <- predict(preProc, trainNA)[,34:(34+preProc$numComp)]
```

# Modelfits
After training the models we apply them to  the remaining data from the trainig data to get an idea of the accuracy.

## Random Forest
```{r rf, cache=T}

(modelfit1 <-train(classe ~., data = trainPC, method = "rf"))
pred1 <- predict(modelfit1, predict(preProc, trainingTest))
confusionMatrix(pred1, trainingTest$classe)
```

## Generalized Boosted Regression Modeling
```{r gbm, cache=T}

(modelfit2 <- train(classe ~., data = trainPC, method = "gbm", verbose = F))
pred2 <- predict(modelfit2, predict(preProc, trainingTest))
confusionMatrix(pred2, trainingTest$classe)
```

## Linear Discriminant Analysis
```{r lda, cache=T}

(modelfit3 <- train(classe ~., data = trainPC, method = "lda"))
pred3 <- predict(modelfit3, predict(preProc, trainingTest))
confusionMatrix(pred3, trainingTest$classe)
```

## Conclusion

Accuracy (out-of-sample-error):

* rf: 98%
* gbm: 80%
* lda: 49%

We choose the **random forest model** because the accuracy of it is that good that we don't expect a better result from combining all three models together.

# Predict test data
Aplly preprocessing and the random forest model to the *testing* dataset.
```{r}
predict(modelfit1, predict(preProc, testing))
```
